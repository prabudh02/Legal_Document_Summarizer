{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "531ae6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import re\n",
    "from gurobipy import *\n",
    "import gzip\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import math\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet_ic, genesis\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from itertools import cycle\n",
    "from operator import itemgetter\n",
    "import math\n",
    "import json\n",
    "from isStatute_isPrecedent import *\n",
    "from mention_statute_sentence import get_statute_mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7ccd4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "WORD = re.compile(r'\\w+')\n",
    "class keyvalue(argparse.Action):\n",
    "    # Constructor calling\n",
    "    def __call__( self , parser, namespace,\n",
    "                 values, option_string = None):\n",
    "        setattr(namespace, self.dest, dict())\n",
    "          \n",
    "        for value in values:\n",
    "            # split it into key and value\n",
    "            key, value = value.split('=')\n",
    "            # assign into dictionary\n",
    "            getattr(namespace, self.dest)[key] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c5af548",
   "metadata": {},
   "outputs": [],
   "source": [
    "WT1 = 1\n",
    "WT2 = 1\n",
    "WT3 = 1\n",
    "cachedstopwords = stopwords.words(\"english\")\n",
    "AUX = ['be','can','cannot','could','am','has','had','is','are','may','might','dare','do','did','have','must','need','ought','shall','should','will','would','shud','cud','don\\'t','didn\\'t','shouldn\\'t','couldn\\'t','wouldn\\'t']\n",
    "NEGATE = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "              \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "              \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "              \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "              \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "              \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "              \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "              \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "POS_TAGS = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a414b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_legal_word(sen,LEG):\n",
    "    temp = []\n",
    "    for k in LEG.keys():\n",
    "        if k in sen.lower() and k not in temp:\n",
    "            temp.append(k)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebefbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from gurobipy import Model, GRB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# You need to define these functions or import them appropriately\n",
    "# from helper_module import get_legal_word, get_statute_mention, should_select, isStatute, isPrecedent\n",
    "\n",
    "POS_TAGS = set([\"NN\", \"NNS\", \"NNP\", \"NNPS\"])\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def optimize(tweet, con_word, ofname, summary_length, num_classes, sentence_limit):\n",
    "    model = Model(\"summary_optimizer\")\n",
    "    model.setParam('OutputFlag', 0)  # Suppress Gurobi output\n",
    "\n",
    "    n = len(tweet)\n",
    "    tweet_var = {}\n",
    "    con_var = {}\n",
    "\n",
    "    for i in range(n):\n",
    "        tweet_var[i] = model.addVar(vtype=GRB.BINARY, name=f\"tweet_{i}\")\n",
    "\n",
    "    for word in con_word.keys():\n",
    "        con_var[word] = model.addVar(vtype=GRB.BINARY, name=f\"con_{word}\")\n",
    "\n",
    "    model.update()\n",
    "\n",
    "    model.addConstr(sum(tweet_var[i] * tweet[i][1] for i in range(n)) <= summary_length, name=\"LengthConstraint\")\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        model.addConstr(sum(tweet_var[i] for i in range(n) if tweet[i][4] == c) <= sentence_limit[c], name=f\"ClassSentenceLimit_{c}\")\n",
    "\n",
    "    for word in con_word.keys():\n",
    "        model.addConstr(con_var[word] <= sum(tweet_var[i] for i in range(n) if word in tweet[i][2]), name=f\"ContentTrigger_{word}\")\n",
    "\n",
    "    for word in con_word.keys():\n",
    "        for i in range(n):\n",
    "            if word in tweet[i][2]:\n",
    "                model.addConstr(tweet_var[i] >= con_var[word], name=f\"ContentEnforcement_{word}_{i}\")\n",
    "\n",
    "    model.setObjective(\n",
    "        sum(con_word[word] * con_var[word] for word in con_word.keys()) +\n",
    "        sum(tweet[i][3] * tweet_var[i] for i in range(n)),\n",
    "        GRB.MAXIMIZE\n",
    "    )\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    selected_sentences = []\n",
    "    for i in range(n):\n",
    "        if tweet_var[i].X > 0.5:\n",
    "            selected_sentences.append((i, tweet[i][0]))\n",
    "\n",
    "    selected_sentences.sort(key=lambda x: x[0])\n",
    "\n",
    "    with open(ofname, 'w', encoding='utf-8') as out_fp:\n",
    "        for _, sentence in selected_sentences:\n",
    "            out_fp.write(sentence + '\\n')\n",
    "\n",
    "def compute_summary(args):\n",
    "    ifname = args.prep_path\n",
    "    SUMMARY_PATH = args.summary_path\n",
    "    CLASS_WEIGHT = args.class_weights\n",
    "    nos = args.class_sents\n",
    "\n",
    "    with open(ifname,'r') as fp:\n",
    "        dic = json.load(fp)\n",
    "\n",
    "    for k,v in CLASS_WEIGHT.items():\n",
    "        print('Weight for class {} is {}'.format(k,v))\n",
    "\n",
    "    print('Total number of documents:{}'.format(len(dic.keys())))\n",
    "    \n",
    "    SUMMARY_LENGTH = {}\n",
    "    with open(args.length_file,'r') as fp:\n",
    "        for l in fp:\n",
    "            wl = l.strip().split('\\t')\n",
    "            if len(wl) != 2:\n",
    "                print(f\"Skipping malformed line in {args.length_file}: {l!r}\")\n",
    "                continue\n",
    "            docid = wl[0].strip().replace(\".txt\", \"\")\n",
    "            SUMMARY_LENGTH[docid] = int(wl[1])\n",
    "\n",
    "    LEGALDICT = {}\n",
    "    with open('dict_words.txt','r') as fp:\n",
    "        for l in fp:\n",
    "            LEGALDICT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "\n",
    "    for k,v in dic.items():\n",
    "        print('Document ID {}'.format(k))\n",
    "        t0 = time.time()\n",
    "        T = {}\n",
    "        TW = {}\n",
    "        index = 0\n",
    "        content_count = {}\n",
    "        CLASS_INDEX = 0\n",
    "        NOS = {}\n",
    "        MAP = {}\n",
    "        Ts = SUMMARY_LENGTH.get(k, 2000)\n",
    "        print('Summary Length: {}'.format(Ts))\n",
    "\n",
    "        for ck,cv in v.items():\n",
    "            CL_WEIGHT = CLASS_WEIGHT.get(ck, 1)\n",
    "            MAP[CLASS_INDEX] = ck\n",
    "            if nos != None and ck in nos.keys():\n",
    "                NOS[CLASS_INDEX] = nos[ck]\n",
    "            else:\n",
    "                NOS[CLASS_INDEX] = args.default_sents\n",
    "\n",
    "            position = 1\n",
    "            for x in cv:\n",
    "                if len(x[0].split()) > 4:\n",
    "                    content = set()\n",
    "                    All = set()\n",
    "                    sentence = x[0]\n",
    "                    tokens = x[2]\n",
    "                    L = 0\n",
    "                    SEN_TEMP = ''\n",
    "                    for y in tokens:\n",
    "                        if y[1] in POS_TAGS:\n",
    "                            L += 1\n",
    "                            All.add(y[0].lower())\n",
    "                            SEN_TEMP += y[0].lower() + ' '\n",
    "                        if y[1] in ['NN', 'NNP']:\n",
    "                            content.add(y[0].lower())\n",
    "                        elif y[1] in ['NNS', 'NNPS']:\n",
    "                            try:\n",
    "                                word = lmtzr.lemmatize(y[0].lower())\n",
    "                            except:\n",
    "                                word = y[0].lower()\n",
    "                            content.add(word)\n",
    "\n",
    "                    LEGAL_WORD = get_legal_word(SEN_TEMP.strip(), LEGALDICT)\n",
    "                    STATUTE_WORD = get_statute_mention(sentence.strip())\n",
    "\n",
    "                    for y in content:\n",
    "                        content_count[y] = args.content_weight\n",
    "                    for y in LEGAL_WORD:\n",
    "                        All.add(y.lower())\n",
    "                        content.add(y.lower())\n",
    "                        content_count[y.lower()] = args.legal_weight\n",
    "                    for y in STATUTE_WORD:\n",
    "                        All.add(y.lower())\n",
    "                        content.add(y.lower())\n",
    "                        content_count[y.lower()] = args.statute_weight\n",
    "\n",
    "                    if should_select(TW, All):\n",
    "                        if ck == 'F':\n",
    "                            score = CL_WEIGHT * (1 / position)\n",
    "                        elif ck == 'S':\n",
    "                            score = CL_WEIGHT * isStatute(sentence, 'current-acts.txt')\n",
    "                        elif ck == 'P':\n",
    "                            score = CL_WEIGHT * isPrecedent(sentence)\n",
    "                        elif ck == 'R':\n",
    "                            score = CL_WEIGHT * position * (isPrecedent(sentence) or isStatute(sentence, 'current-acts.txt'))\n",
    "                        else:\n",
    "                            score = CL_WEIGHT\n",
    "\n",
    "                        T[index] = [sentence, content, L, score, CLASS_INDEX]\n",
    "                        TW[index] = All\n",
    "                        index += 1\n",
    "\n",
    "                    position += 1\n",
    "\n",
    "            CLASS_INDEX += 1\n",
    "\n",
    "        L = len(T.keys())\n",
    "        print('Number of tweets: {}'.format(L))\n",
    "\n",
    "        tweet_cur_window = {\n",
    "            i: [T[i][0].strip(), int(T[i][2]), T[i][1], float(T[i][3]), int(T[i][4])]\n",
    "            for i in range(L)\n",
    "        }\n",
    "\n",
    "        print('Number of classes: ', CLASS_INDEX)\n",
    "        print('Sentence Limit: {}'.format(NOS))\n",
    "        print('Class Mapping: {}'.format(MAP))\n",
    "\n",
    "        ofname = os.path.join(SUMMARY_PATH, k + '.txt')\n",
    "        optimize(tweet_cur_window, content_count, ofname, Ts, CLASS_INDEX, NOS)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print('Summarization done: ', ofname, ' ', t1 - t0)\n",
    "\n",
    "    print('Done with documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c49cbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_select(T,new):\n",
    "    if len(new)==0:\n",
    "        return 0\n",
    "    for i in range(0,len(T),1):\n",
    "        temp = T[i]\n",
    "        common = set(temp).intersection(set(new))\n",
    "        if len(common)==len(new):\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0cf16408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weight(P,L,U):\n",
    "    min_p = min(P.values())\n",
    "    max_p = max(P.values())\n",
    "\n",
    "    x = U - L + 4.0 - 4.0\n",
    "    y = max_p - min_p + 4.0 - 4.0\n",
    "    factor = round(x/y,4)\n",
    "\n",
    "    mod_P = {}\n",
    "    for k,v in P.iteritems():\n",
    "        val = L + factor * (v - min_p)\n",
    "        mod_P[k] = round(val,4)\n",
    "\n",
    "    count = 0\n",
    "    return mod_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import Model, GRB, LinExpr, GurobiError\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "def optimize(tweet, con_weight, ofname, L, CLASS_INDEX, NOS):\n",
    "    con_word = {}\n",
    "    tweet_word = {}\n",
    "    tweet_index = 1\n",
    "    for k, v in tweet.items():\n",
    "        set_of_words = v[2]\n",
    "        for x in set_of_words:\n",
    "            if x not in con_word:\n",
    "                p1 = round(con_weight.get(x, 0.0), 4) * WT2  \n",
    "                con_word[x] = p1\n",
    "\n",
    "        tweet_word[tweet_index] = [v[1], set_of_words, v[0], v[3], v[4]]  \n",
    "        tweet_index += 1\n",
    "\n",
    "    sen = list(tweet_word.keys())\n",
    "    sen.sort()\n",
    "    entities = list(con_word.keys())\n",
    "    print('Length: ', len(sen), len(entities))\n",
    "\n",
    "\n",
    "    m = Model(\"sol1\")\n",
    "\n",
    "    sen_var = [m.addVar(vtype=GRB.BINARY, name=f\"x{i+1}\") for i in range(len(sen))]\n",
    "\n",
    "    con_var = [m.addVar(vtype=GRB.BINARY, name=f\"y{i+1}\") for i in range(len(entities))]\n",
    "\n",
    "    m.update()\n",
    "\n",
    "    P = LinExpr() \n",
    "    C1 = LinExpr()  \n",
    "    C4 = LinExpr()  \n",
    "    C2 = [] \n",
    "    counter = -1\n",
    "\n",
    "    for i in range(len(sen)):\n",
    "        P += tweet_word[i+1][3] * sen_var[i]\n",
    "        C1 += tweet_word[i+1][0] * sen_var[i]\n",
    "        v = tweet_word[i+1][1] \n",
    "        C = LinExpr()\n",
    "        flag = 0\n",
    "        for j in range(len(entities)):\n",
    "            if entities[j] in v:\n",
    "                flag += 1\n",
    "                C += con_var[j]\n",
    "        if flag > 0:\n",
    "            counter += 1\n",
    "            m.addConstr(C >= flag * sen_var[i], f\"c{counter}\")\n",
    "                \n",
    "    for i in range(len(entities)):\n",
    "        P += con_word[entities[i]] * con_var[i]\n",
    "        C = LinExpr()\n",
    "        flag = 0\n",
    "        for j in range(len(sen)):\n",
    "            v = tweet_word[j+1][1]\n",
    "            if entities[i] in v:\n",
    "                flag = 1\n",
    "                C += sen_var[j]\n",
    "        if flag == 1:\n",
    "            counter += 1\n",
    "            m.addConstr(C >= con_var[i], f\"c{counter}\")\n",
    "\n",
    "    CC = 0\n",
    "    while CC < CLASS_INDEX:\n",
    "        C = LinExpr()\n",
    "        for i in range(len(sen)):\n",
    "            if tweet_word[i+1][4] == CC:\n",
    "                C += sen_var[i]\n",
    "        counter += 1\n",
    "        m.addConstr(C >= NOS[CC], f\"c{counter}\")\n",
    "        CC += 1\n",
    "\n",
    "    counter += 1\n",
    "    m.addConstr(C1 <= L, f\"c{counter}\")\n",
    "\n",
    "    m.setObjective(P, GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "    fo = codecs.open(ofname, 'w', 'utf-8')\n",
    "    try:\n",
    "        m.optimize()\n",
    "        #print('vars: {}'.format(m.getVars()))\n",
    "        for v in m.getVars():\n",
    "            if v.x == 1:\n",
    "                temp = v.varName.split('x')\n",
    "                if len(temp) == 2:\n",
    "                    X = ''\n",
    "                    fo.write(tweet_word[int(temp[1])][2])  \n",
    "                    fo.write('\\n')\n",
    "    except GurobiError as e:\n",
    "        print(e)\n",
    "        sys.exit(0)\n",
    "\n",
    "    fo.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd22daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_NEW(word,tweet_count,PLACE):\n",
    "    score = {}\n",
    "    discard = []\n",
    "    THR = 5\n",
    "    N = tweet_count + 4.0 - 4.0\n",
    "    for k,v in word.iteritems():\n",
    "        D = k.split('_')\n",
    "        D_w = D[0].strip(' \\t\\n\\r')\n",
    "        D_t = D[1].strip(' \\t\\n\\r')\n",
    "        if D_w not in discard:\n",
    "            tf = v\n",
    "            w = 1 + math.log(tf,2)\n",
    "            df = v + 4.0 - 4.0\n",
    "            try:\n",
    "                y = round(N/df,4)\n",
    "                idf = math.log10(y)\n",
    "            except Exception as e:\n",
    "                idf = 0\n",
    "            val = round(w * idf, 4)\n",
    "            if D_t=='P' and tf>=THR:\n",
    "                score[k] = val\n",
    "            elif tf>=THR and D_t=='S':\n",
    "                score[k] = val\n",
    "            elif tf>=THR and len(D_w)>2:\n",
    "                score[k] = val\n",
    "            else:\n",
    "                score[k] = 0\n",
    "        else:\n",
    "            score[k] = 0\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50e911ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numToWord(number):\n",
    "    word = []\n",
    "    if number < 0 or number > 999999:\n",
    "        return number\n",
    "        # raise ValueError(\"You must type a number between 0 and 999999\")\n",
    "    ones = [\"\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\",\"eleven\",\"twelve\",\"thirteen\",\"fourteen\",\"fifteen\",\"sixteen\",\"seventeen\",\"eighteen\",\"nineteen\"]\n",
    "    if number == 0: return \"zero\"\n",
    "    if number > 9 and number < 20:\n",
    "        return ones[number]\n",
    "    tens = [\"\",\"ten\",\"twenty\",\"thirty\",\"forty\",\"fifty\",\"sixty\",\"seventy\",\"eighty\",\"ninety\"]\n",
    "    word.append(ones[int(str(number)[-1])])\n",
    "    if number >= 10:\n",
    "        word.append(tens[int(str(number)[-2])])\n",
    "    if number >= 100:\n",
    "        word.append(\"hundred\")\n",
    "        word.append(ones[int(str(number)[-3])])\n",
    "    if number >= 1000 and number < 1000000:\n",
    "        word.append(\"thousand\")\n",
    "        word.append(numToWord(int(str(number)[:-3])))\n",
    "    for i,value in enumerate(word):\n",
    "        if value == '':\n",
    "            word.pop(i)\n",
    "    return ' '.join(word[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f6d7611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class L1 is 2\n",
      "Weight for class L2 is 3\n",
      "Total number of documents:3\n",
      "Document ID 1253\n",
      "Summary Length: 170\n",
      "Number of tweets: 92\n",
      "Number of classes:  7\n",
      "Sentence Limit: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1}\n",
      "Class Mapping: {0: 'F', 1: 'R', 2: 'S', 3: 'A', 4: 'RLC', 5: 'P', 6: 'RPC'}\n",
      "Length:  92 303\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[arm] - Darwin 24.3.0 24D81)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 403 rows, 395 columns and 2661 nonzeros\n",
      "Model fingerprint: 0x0d627896\n",
      "Variable types: 0 continuous, 395 integer (395 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+01]\n",
      "  Objective range  [6e-02, 4e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "Found heuristic solution: objective 72.0666667\n",
      "Presolve removed 211 rows and 202 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 192 rows, 193 columns, 1551 nonzeros\n",
      "Variable types: 0 continuous, 193 integer (193 binary)\n",
      "Found heuristic solution: objective 146.0666667\n",
      "\n",
      "Root relaxation: objective 4.229278e+02, 93 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  422.92778    0   15  146.06667  422.92778   190%     -    0s\n",
      "H    0     0                     413.2833333  422.92778  2.33%     -    0s\n",
      "H    0     0                     417.2833333  422.92778  1.35%     -    0s\n",
      "H    0     0                     418.7416667  422.92778  1.00%     -    0s\n",
      "H    0     0                     420.6166667  422.92778  0.55%     -    0s\n",
      "     0     0  422.19444    0   30  420.61667  422.19444  0.38%     -    0s\n",
      "     0     0  422.19444    0    6  420.61667  422.19444  0.38%     -    0s\n",
      "H    0     0                     420.6236111  422.19444  0.37%     -    0s\n",
      "H    0     0                     421.3500000  422.19444  0.20%     -    0s\n",
      "H    0     0                     421.3944444  422.19444  0.19%     -    0s\n",
      "     0     0  422.16368    0    9  421.39444  422.16368  0.18%     -    0s\n",
      "H    0     0                     421.4611111  422.16368  0.17%     -    0s\n",
      "H    0     0                     421.4944444  421.78333  0.07%     -    0s\n",
      "     0     0  421.78333    0   12  421.49444  421.78333  0.07%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Cover: 1\n",
      "  Clique: 1\n",
      "  MIR: 2\n",
      "  Flow cover: 1\n",
      "  Zero half: 2\n",
      "\n",
      "Explored 1 nodes (125 simplex iterations) in 0.03 seconds (0.01 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 10: 421.494 421.461 421.394 ... 413.283\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.214944444444e+02, best bound 4.214944444444e+02, gap 0.0000%\n",
      "Summarization done:  summaries/1253.txt   0.861342191696167\n",
      "Document ID 4042\n",
      "Summary Length: 80\n",
      "Number of tweets: 61\n",
      "Number of classes:  4\n",
      "Sentence Limit: {0: 1, 1: 1, 2: 1, 3: 1}\n",
      "Class Mapping: {0: 'F', 1: 'R', 2: 'A', 3: 'RPC'}\n",
      "Length:  61 214\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[arm] - Darwin 24.3.0 24D81)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 280 rows, 275 columns and 1717 nonzeros\n",
      "Model fingerprint: 0xe41a017a\n",
      "Variable types: 0 continuous, 275 integer (275 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+01]\n",
      "  Objective range  [3e-02, 5e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 8e+01]\n",
      "Found heuristic solution: objective 26.0526316\n",
      "Presolve removed 140 rows and 136 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 140 rows, 139 columns, 1100 nonzeros\n",
      "Variable types: 0 continuous, 139 integer (139 binary)\n",
      "Found heuristic solution: objective 32.0526316\n",
      "\n",
      "Root relaxation: objective 2.267331e+02, 86 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  226.73305    0    5   32.05263  226.73305   607%     -    0s\n",
      "H    0     0                     224.2145345  226.73305  1.12%     -    0s\n",
      "H    0     0                     225.2671661  226.73305  0.65%     -    0s\n",
      "H    0     0                     226.2145345  226.73305  0.23%     -    0s\n",
      "     0     0  226.73305    0    5  226.21453  226.73305  0.23%     -    0s\n",
      "\n",
      "Explored 1 nodes (86 simplex iterations) in 0.01 seconds (0.01 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 226.215 225.267 224.215 ... 26.0526\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.262145344851e+02, best bound 2.262145344851e+02, gap 0.0000%\n",
      "Summarization done:  summaries/4042.txt   0.3876659870147705\n",
      "Document ID 899\n",
      "Summary Length: 180\n",
      "Number of tweets: 84\n",
      "Number of classes:  7\n",
      "Sentence Limit: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1}\n",
      "Class Mapping: {0: 'F', 1: 'RLC', 2: 'R', 3: 'S', 4: 'P', 5: 'A', 6: 'RPC'}\n",
      "Length:  84 361\n",
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (mac64[arm] - Darwin 24.3.0 24D81)\n",
      "\n",
      "CPU model: Apple M1\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Optimize a model with 453 rows, 445 columns and 2839 nonzeros\n",
      "Model fingerprint: 0x6b958ced\n",
      "Variable types: 0 continuous, 445 integer (445 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 5e+01]\n",
      "  Objective range  [5e-02, 4e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "Found heuristic solution: objective 92.1213235\n",
      "Presolve removed 244 rows and 237 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 209 rows, 208 columns, 1781 nonzeros\n",
      "Variable types: 0 continuous, 208 integer (208 binary)\n",
      "Found heuristic solution: objective 98.3713235\n",
      "\n",
      "Root relaxation: objective 3.646004e+02, 98 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  364.60038    0   10   98.37132  364.60038   271%     -    0s\n",
      "H    0     0                     359.1458333  364.60038  1.52%     -    0s\n",
      "H    0     0                     363.2367424  364.60038  0.38%     -    0s\n",
      "*    0     0               0     364.1458333  364.14583  0.00%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 1\n",
      "  Cover: 3\n",
      "  MIR: 3\n",
      "  StrongCG: 2\n",
      "  GUB cover: 1\n",
      "  RLT: 1\n",
      "\n",
      "Explored 1 nodes (108 simplex iterations) in 0.02 seconds (0.01 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 364.146 363.237 359.146 ... 92.1213\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.641458333333e+02, best bound 3.641458333333e+02, gap 0.0000%\n",
      "Summarization done:  summaries/899.txt   0.7971720695495605\n",
      "Done with documents\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def main():\n",
    "    args = Namespace(\n",
    "        prep_path=\"prepared_data.json\",\n",
    "        summary_path=\"summaries\",\n",
    "        length_file=\"length_file.txt\",\n",
    "        class_weights={\"L1\": 2, \"L2\": 3},\n",
    "        content_weight=1,\n",
    "        legal_weight=3,\n",
    "        statute_weight=5,\n",
    "        class_sents={\"L1\": 2, \"L2\": 1},\n",
    "        default_sents=1\n",
    "    )\n",
    "    \n",
    "    compute_summary(args)\n",
    "    print('Done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
